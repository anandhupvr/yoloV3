{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anandhu/anaconda3/envs/mlenv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# from utils import util\n",
    "\n",
    "\n",
    "class Anet:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.x = tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=[\n",
    "                None,\n",
    "                config[\"IMAGE_W\"],\n",
    "                config[\"IMAGE_H\"],\n",
    "                3])\n",
    "\n",
    "\n",
    "    def network(self):\n",
    "            # Layer 1\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            self.x,\n",
    "            filters=32,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_1\")\n",
    "        conv1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            name=\"norm_1\")\n",
    "        maxpool1 = tf.nn.max_pool(\n",
    "            conv1,\n",
    "            ksize=[1, 2, 2, 1],\n",
    "            strides=[1, 2, 2, 1],\n",
    "            padding=\"SAME\")\n",
    "        # Layer 2\n",
    "\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            maxpool1,\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_2\")\n",
    "        conv2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            name=\"norm_2\")\n",
    "        maxpool2 = tf.nn.max_pool(\n",
    "            conv2,\n",
    "            ksize=[1, 2, 2, 1],\n",
    "            strides=[1, 2, 2, 1],\n",
    "            padding=\"SAME\")\n",
    "        # Layer 3\n",
    "\n",
    "        conv3 = tf.layers.conv2d(\n",
    "            maxpool2, filters=128,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_3\")\n",
    "        conv3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            name=\"norm_3\")\n",
    "\n",
    "        # Layer 4\n",
    "        conv4 = tf.layers.conv2d(\n",
    "            conv3, filters=64,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_4\")\n",
    "        conv4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            name=\"norm_4\")\n",
    "\n",
    "        # Layer 5\n",
    "        conv5 = tf.layers.conv2d(\n",
    "            conv4,\n",
    "            filters=128,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_5\")\n",
    "        conv5 = tf.layers.batch_normalization(\n",
    "            conv5,\n",
    "            name=\"norm_5\")\n",
    "        max_pool3 = tf.nn.max_pool(\n",
    "            conv5,\n",
    "            ksize=[1, 2, 2, 1],\n",
    "            strides=[1, 2, 2, 1],\n",
    "            padding=\"SAME\")\n",
    "\n",
    "        # Layer 6\n",
    "        conv6 = tf.layers.conv2d(\n",
    "            max_pool3,\n",
    "            filters=256,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_6\")\n",
    "        conv6 = tf.layers.batch_normalization(\n",
    "            conv6,\n",
    "            name=\"norm_6\")\n",
    "\n",
    "        # Layer 7\n",
    "        conv7 = tf.layers.conv2d(\n",
    "            conv6,\n",
    "            filters=128,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_7\")\n",
    "        conv7 = tf.layers.batch_normalization(\n",
    "            conv7,\n",
    "            name=\"norm_7\")\n",
    "\n",
    "        # Layer 8\n",
    "        conv8 = tf.layers.conv2d(\n",
    "            conv7,\n",
    "            filters=256,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_8\")\n",
    "        conv8 = tf.layers.batch_normalization(\n",
    "            conv8,\n",
    "            name=\"norm_8\")\n",
    "        max_pool4 = tf.nn.max_pool(\n",
    "            conv8,\n",
    "            ksize=[1, 2, 2, 1],\n",
    "            strides=[1, 2, 2, 1],\n",
    "            padding=\"SAME\")\n",
    "\n",
    "        # Layer 9\n",
    "        conv9 = tf.layers.conv2d(\n",
    "            max_pool4,\n",
    "            filters=512,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_9\")\n",
    "        conv9 = tf.layers.batch_normalization(\n",
    "            conv9,\n",
    "            name=\"norm_9\")\n",
    "\n",
    "        # Layer 10\n",
    "        conv10 = tf.layers.conv2d(\n",
    "            conv9,\n",
    "            filters=256,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_10\")\n",
    "        conv10 = tf.layers.batch_normalization(\n",
    "            conv10,\n",
    "            name=\"norm_10\")\n",
    "\n",
    "        # Layer 11\n",
    "        conv11 = tf.layers.conv2d(\n",
    "            conv10,\n",
    "            filters=512,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_11\")\n",
    "        conv11 = tf.layers.batch_normalization(\n",
    "            conv11,\n",
    "            name=\"norm_11\")\n",
    "\n",
    "        # Layer 12\n",
    "        conv12 = tf.layers.conv2d(\n",
    "            conv11,\n",
    "            filters=256,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_12\")\n",
    "        conv12 = tf.layers.batch_normalization(\n",
    "            conv12,\n",
    "            name=\"norm_12\")\n",
    "\n",
    "        # Layer 13\n",
    "        conv13 = tf.layers.conv2d(\n",
    "            conv12,\n",
    "            filters=512,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_13\")\n",
    "        conv13 = tf.layers.batch_normalization(\n",
    "            conv12,\n",
    "            name=\"norm_13\")\n",
    "\n",
    "        skip_connection = conv13\n",
    "\n",
    "        max_pool5 = tf.nn.max_pool(\n",
    "            conv13,\n",
    "            ksize=[1, 2, 2, 1],\n",
    "            strides=[1, 2, 2, 1],\n",
    "            padding=\"SAME\")\n",
    "\n",
    "        # Layer 14\n",
    "        conv14 = tf.layers.conv2d(\n",
    "            max_pool5,\n",
    "            filters=1024,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_14\")\n",
    "        conv14 = tf.layers.batch_normalization(\n",
    "            conv14,\n",
    "            name=\"norm_14\")\n",
    "\n",
    "        # Layer 15\n",
    "        conv15 = tf.layers.conv2d(\n",
    "            conv14,\n",
    "            filters=512,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_15\")\n",
    "\n",
    "        conv15 = tf.layers.batch_normalization(\n",
    "            conv15,\n",
    "            name=\"norm_15\")\n",
    "\n",
    "        # Layer 16\n",
    "        conv16 = tf.layers.conv2d(\n",
    "            conv15,\n",
    "            filters=1024,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_16\")\n",
    "        conv16 = tf.layers.batch_normalization(\n",
    "            conv16,\n",
    "            name=\"norm_16\")\n",
    "\n",
    "        # Layer 17\n",
    "        conv17 = tf.layers.conv2d(\n",
    "            conv16,\n",
    "            filters=512,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_17\")\n",
    "        conv17 = tf.layers.batch_normalization(\n",
    "            conv16,\n",
    "            name=\"norm_17\")\n",
    "\n",
    "        # Layer 18\n",
    "        conv18 = tf.layers.conv2d(\n",
    "            conv17,\n",
    "            filters=1024,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_18\")\n",
    "        conv18 = tf.layers.batch_normalization(\n",
    "            conv18,\n",
    "            name=\"norm_18\")\n",
    "\n",
    "        # Layer 19\n",
    "        conv19 = tf.layers.conv2d(\n",
    "            conv18,\n",
    "            filters=1024,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_19\")\n",
    "        conv19 = tf.layers.batch_normalization(\n",
    "            conv19,\n",
    "            name=\"norm_19\")\n",
    "\n",
    "        # Layer 20\n",
    "        conv20 = tf.layers.conv2d(\n",
    "            conv19,\n",
    "            filters=1024,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_20\")\n",
    "        conv20 = tf.layers.batch_normalization(\n",
    "            conv20,\n",
    "            name=\"norm_20\")\n",
    "\n",
    "        # Layer 21\n",
    "\n",
    "        conv21 = tf.layers.conv2d(\n",
    "            skip_connection,\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_21\")\n",
    "        skip_connection = tf.layers.batch_normalization(\n",
    "            conv21,\n",
    "            name=\"norm_21\")\n",
    "        # skip_connection = space_to_depth_x2(skip_connection)\n",
    "        skip_connection = tf.space_to_depth(skip_connection, block_size=2)\n",
    "\n",
    "        route = tf.concat(\n",
    "            [skip_connection,\n",
    "             conv20],\n",
    "             3)\n",
    "\n",
    "        # Layer 22\n",
    "\n",
    "        conv22 = tf.layers.conv2d(\n",
    "            route,\n",
    "            filters=1024,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_22\")\n",
    "        conv22 = tf.layers.batch_normalization(\n",
    "            conv22,\n",
    "            name=\"norm_22\")\n",
    "\n",
    "        # Layer 23\n",
    "        conv23 = tf.layers.conv2d(\n",
    "            conv22,\n",
    "            filters=self.config[\"BOX\"] * (4 + 1 + self.config[\"CLASS\"]),\n",
    "            kernel_size=[1, 1],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_23\")\n",
    "        output = tf.reshape(\n",
    "            conv23,\n",
    "            [\n",
    "                self.config[\"BATCH_SIZE\"],\n",
    "                self.config[\"GRID_H\"],\n",
    "                self.config[\"GRID_W\"],\n",
    "                self.config[\"BOX\"],\n",
    "                4 + 1 + self.config[\"CLASS\"]\n",
    "            ])\n",
    "        return output\n",
    "    def getX(self):\n",
    "        return self.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = getParams()\n",
    "arch = Anet(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = arch.network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'conv_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-38ab093d32e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'conv_1'"
     ]
    }
   ],
   "source": [
    "layers = {v.op.name: v for v in tf.trainable_variables()}\n",
    "print (layers['conv_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = [v for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tf.trainable_variables(scope=\"norm_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/run/media/anandhu/baskaran/agrima/git_repos/yoloV2/yolov2-voc_100.weights\"\n",
    "class weight_reader:\n",
    "    def __init__(self, weight_file):\n",
    "        self.offset = 4\n",
    "        self.all_weights = np.fromfile(weight_file)\n",
    "\n",
    "    def read_bytes(self, size):\n",
    "        self.offset = self.offset + size\n",
    "        return all_weights[self.offset - size:self.offset]\n",
    "\n",
    "    def reset(self):\n",
    "        self.offset = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 4\n",
    "all_weights = np.fromfile(path)\n",
    "offset = offset + 19\n",
    "weights = all_weights[offset - 19:offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_load(weights, var_list):\n",
    "    ptr = 0\n",
    "    i = 0\n",
    "    assign_ops = []\n",
    "    while i < len(var_list) - 1:\n",
    "            var1 = var_list[i]\n",
    "            var2 = var_list[i + 1]\n",
    "            if \"conv\" in var1.name.split(\"/\")[-1]:\n",
    "                if \"norm\" in var2.name.split(\"/\")[-1]:\n",
    "                    gamma, beta, mean, var = var_list[i + 1:i + 5]\n",
    "                    batch_norm_vars = [beta, gamma, mean, var]\n",
    "                    for var in batch_norm_vars:\n",
    "                        shape = var.shape.as_list()\n",
    "                        num_params = np.prod(shape)\n",
    "                        var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
    "                        ptr += num_params\n",
    "                        asign_ops.append(tf.assign(var, var_weights, validate_shape=True))\n",
    "                    i += 4\n",
    "                elif \"conv\" in var2.name.split(\"/\")[-2]:\n",
    "                    bias = var2\n",
    "                    bias_shape = bias_shpae.as_list()\n",
    "                    bias_params - np.prod(bias_shape)\n",
    "                    bias_weights = weights[ptr:ptr + bias_params].reshape(bias_shape)\n",
    "                    ptr += bias_params\n",
    "                    assign_ops.append(tf.assign(bias, bias_weights, validate_shape=True))\n",
    "\n",
    "                    i += 1\n",
    "                shape = var1.shape.as_list()\n",
    "                num_params = np.prod(shape)\n",
    "\n",
    "                var_weights = weights[ptr:ptr + num_params].reshape((shape[3], shape[2], shape[0], shape[1]))\n",
    "                var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
    "                ptr += num_params\n",
    "                assign_ops.append(tf.assign(var1, var_weights, validate_shape=True))\n",
    "                i += 1\n",
    "    return assign_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = weight_load(weights, var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.variable_scope(\"conv_1\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'variable_scope' object has no attribute 'get_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-9122f9374336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'variable_scope' object has no attribute 'get_params'"
     ]
    }
   ],
   "source": [
    "g.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "LABELS = ['apple', 'banana']\n",
    "IMAGE_H, IMAGE_W = 416, 416\n",
    "GRID_H, GRID_W = 13, 13\n",
    "BOX = 5\n",
    "CLASS = len(LABELS)\n",
    "CLASS_WEIGHTS = np.ones(CLASS, dtype='float32')\n",
    "OBJ_THRESHOLD = 0.3  # 0.5\n",
    "NMS_THRESHOLD = 0.3  # 0.45\n",
    "ANCHORS = [\n",
    "    0.57273,\n",
    "    0.677385,\n",
    "    1.87446,\n",
    "    2.06253,\n",
    "    3.33843,\n",
    "    5.47434,\n",
    "    7.88282,\n",
    "    3.52778,\n",
    "    9.77052,\n",
    "    9.16828]\n",
    "NO_OBJECT_SCALE = 1.0\n",
    "OBJECT_SCALE = 5.0\n",
    "COORD_SCALE = 1.0\n",
    "CLASS_SCALE = 1.0\n",
    "BATCH_SIZE = 10\n",
    "WARM_UP_BATCHES = 0\n",
    "TRUE_BOX_BUFFER = 50\n",
    "\n",
    "\n",
    "def getParams():\n",
    "    config = {\n",
    "        'IMAGE_H': IMAGE_H,\n",
    "        'IMAGE_W': IMAGE_W,\n",
    "        'GRID_H': GRID_H,\n",
    "        'GRID_W': GRID_W,\n",
    "        'BOX': BOX,\n",
    "        'LABELS': LABELS,\n",
    "        'CLASS': len(LABELS),\n",
    "        'ANCHORS': ANCHORS,\n",
    "        'BATCH_SIZE': BATCH_SIZE,\n",
    "        'OBJECT_SCALE': OBJECT_SCALE,\n",
    "        'NO_OBJECT_SCALE': NO_OBJECT_SCALE,\n",
    "        'COORD_SCALE': COORD_SCALE,\n",
    "        'CLASS_SCALE': CLASS_SCALE,\n",
    "        'TRUE_BOX_BUFFER': TRUE_BOX_BUFFER\n",
    "    }\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
